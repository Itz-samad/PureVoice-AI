{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pyaudio\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from scipy.signal import butter, lfilter\n",
    "import wave\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record 30 seconds of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording 30 seconds of audio...\n",
      "Recording complete.\n",
      "Audio saved to recordings/user_voice.wav\n"
     ]
    }
   ],
   "source": [
    "# Directory for storing recorded audio\n",
    "os.makedirs(\"recordings\", exist_ok=True)\n",
    "\n",
    "def record_audio(filename, duration=30, rate=16000, channels=1):\n",
    "    \"\"\"Records audio for the specified duration and saves it to a .wav file.\"\"\"\n",
    "    chunk = 1024\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=channels,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk)\n",
    "    print(f\"Recording {duration} seconds of audio...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "    print(\"Recording complete.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Save audio as a .wav file using wave module\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "    print(f\"Audio saved to {filename}\")\n",
    "\n",
    "record_audio(\"recordings/user_voice.wav\", duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Audio: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=16000)  # Load audio file\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Extract MFCC features\n",
    "    mfccs = np.mean(mfccs.T, axis=0)  # Average over time to get a fixed-size feature vector\n",
    "    return mfccs\n",
    "\n",
    "user_features = extract_features(\"recordings/user_voice.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simple Neural Network for Voice Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN layers for feature extraction\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    # LSTM layers for sequential data\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(64))\n",
    "\n",
    "    # Fully connected layer for classification\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification: Voice vs. Background noise\n",
    "   \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare training data (simplified example, replace with a proper dataset)\n",
    "X_train = np.array([user_features, np.random.rand(13)])  # User voice + random noise\n",
    "y_train = np.array([0, 1])  # Labels: user (1), not user (0)\n",
    "\n",
    "model = build_model(input_shape=(13, 1))\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"voice_recognition_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model(\"voice_recognition_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Suppression: Simple Spectral Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise suppression complete.\n"
     ]
    }
   ],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    \"\"\"Creates a bandpass filter.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=300, highcut=3400, fs=16000, order=5):\n",
    "    \"\"\"Applies a bandpass filter to audio data.\"\"\"\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "def noise_suppression(audio_path):\n",
    "    \"\"\"Performs basic noise suppression on an audio file.\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "    filtered_audio = bandpass_filter(audio)\n",
    "    sf.write(\"recordings/filtered_audio.wav\", filtered_audio, sr)\n",
    "    print(\"Noise suppression complete.\")\n",
    "\n",
    "\n",
    "\n",
    "noise_suppression(\"recordings/user_voice.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Noise Suppression and Voice Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def real_time_processing():\n",
    "    \"\"\"Processes real-time audio for voice recognition and noise suppression.\"\"\"\n",
    "    chunk = 1024\n",
    "    rate = 16000\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk)\n",
    "    print(\"Listening for user's voice...\")\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(chunk, exception_on_overflow=False)  # Avoid overflow errors\n",
    "            audio = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            filtered_audio = bandpass_filter(audio, fs=rate)\n",
    "            \n",
    "            # Use trained model to detect the user's voice\n",
    "            mfccs = librosa.feature.mfcc(y=filtered_audio, sr=rate, n_mfcc=13)\n",
    "            mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "            prediction = model.predict(np.expand_dims(mfccs_mean, axis=0))\n",
    "            \n",
    "            if prediction > 0.5:\n",
    "                print(\"\\n User's voice detected.\")\n",
    "            # elif prediction < 0.5:\n",
    "                # print(\"\\n User's voice not detected.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "real_time_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
